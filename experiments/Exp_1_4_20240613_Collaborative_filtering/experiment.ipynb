{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation (loading and preprocessing)\n",
    "\n",
    "First of all we need to load our dataset (See `README.md` for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('../../data/ml-1m/movies.dat', sep='::', header=None, names=['movie_id', 'title', 'genres'], engine='python', encoding='latin-1')\n",
    "ratings = pd.read_csv('../../data/ml-1m/ratings.dat', sep='::', header=None, names=['user_id', 'movie_id', 'rating', 'timestamp'], engine='python', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-to-user collaborative filtering\n",
    "\n",
    "Sources: The weighted average algorithm was inspired by [this](https://medium.com/analytics-vidhya/recommendation-system-using-collaborative-filtering-cc310e641fde) Medium article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ratings_train, X_ratings_test, y_ratings_train, y_ratings_test = train_test_split(ratings, ratings['user_id'], stratify=ratings['user_id'], test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ratings_train_by_user = X_ratings_train.pivot(index='user_id', columns='movie_id', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movie_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>3943</th>\n",
       "      <th>3944</th>\n",
       "      <th>3945</th>\n",
       "      <th>3946</th>\n",
       "      <th>3947</th>\n",
       "      <th>3948</th>\n",
       "      <th>3949</th>\n",
       "      <th>3950</th>\n",
       "      <th>3951</th>\n",
       "      <th>3952</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3679 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movie_id  1     2     3     4     5     6     7     8     9     10    ...  \\\n",
       "user_id                                                               ...   \n",
       "1          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "2          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "3          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "5          NaN   NaN   NaN   NaN   NaN   2.0   NaN   NaN   NaN   NaN  ...   \n",
       "\n",
       "movie_id  3943  3944  3945  3946  3947  3948  3949  3950  3951  3952  \n",
       "user_id                                                               \n",
       "1          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "5          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 3679 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ratings_train_by_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_dummy = X_ratings_train_by_user.copy().fillna(0)\n",
    "similarity_matrix = cosine_similarity(df_ratings_dummy, df_ratings_dummy)\n",
    "similarity_matrix_df = pd.DataFrame(similarity_matrix, index=df_ratings_dummy.index, columns=df_ratings_dummy.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighed_avg_rating_for_movie(id_movie, id_user):\n",
    "\n",
    "    if not id_movie in X_ratings_train_by_user:\n",
    "        return 2.5 #average\n",
    "    cosine_scores = similarity_matrix_df[id_user] #similarity of id_user with every other user\n",
    "    ratings_scores = X_ratings_train_by_user[id_movie]      #ratings of every other user for the movie id_movie\n",
    "    #won't consider users who havent rated id_movie so drop similarity scores and ratings corresponsing to np.nan\n",
    "    index_not_rated = ratings_scores[ratings_scores.isnull()].index\n",
    "    ratings_scores = ratings_scores.dropna()\n",
    "    cosine_scores = cosine_scores.drop(index_not_rated)\n",
    "    #calculating rating by weighted mean of ratings and cosine scores of the users who have rated the movie\n",
    "    ratings_movie = np.dot(ratings_scores, cosine_scores)/cosine_scores.sum()\n",
    "\n",
    "    return ratings_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average rating for movie #150 for user #350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.086058296388687"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighed_avg_rating_for_movie(150, 350) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_on_test_set(X_test):\n",
    "    user_movie_pairs = zip(X_test['movie_id'], X_test['user_id'])\n",
    "    predicted_ratings = np.array([weighed_avg_rating_for_movie(movie, user) for (movie,user) in user_movie_pairs])\n",
    "    true_ratings = np.array(X_test['rating'])\n",
    "    score = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--(array([3, 4, 3, 2, 1, 2, 2, 1, 5, 4]), array([3.16042448, 3.74945348, 3.77095095, 3.92528707, 3.22930641,\n",
      "       1.63100507, 3.2415709 , 3.72150037, 3.80263468, 4.24424964]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.976425275471315"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_on_test_set(X_ratings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering evaluation\n",
    "\n",
    "**Note:** before reading this section, it is highly adviced to first read though the `1_5_Evaluation_methodology` experiment, and also both baseline experiments, as there are a lot of overview and evaluation insights.\n",
    "\n",
    "## User-to-user collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "PROJECT_DIR = '../../'\n",
    "PROJECT_DIR = osp.abspath(PROJECT_DIR)\n",
    "print(PROJECT_DIR in sys.path)\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    print(f'Adding project directory to the sys.path: {PROJECT_DIR!r}')\n",
    "    sys.path.insert(1, PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.abstract_rs_model import AbstractRSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import EvaluationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilteringUUModel(AbstractRSModel):\n",
    "    def _weighted_avg_rating_for_movie(self, train_data, similarity_matrix_df, id_movie, id_user):\n",
    "        if not id_movie in train_data or not id_user in similarity_matrix_df:\n",
    "            return 2.5 #average\n",
    "        cosine_scores = similarity_matrix_df[id_user] #similarity of id_user with every other user\n",
    "        ratings_scores = train_data[id_movie]      #ratings of every other user for the movie id_movie\n",
    "        #won't consider users who havent rated id_movie so drop similarity scores and ratings corresponsing to np.nan\n",
    "        index_not_rated = ratings_scores[ratings_scores.isnull()].index\n",
    "        # print(f'--index_not_rated {index_not_rated} {len(cosine_scores)} {len(ratings_scores)}')\n",
    "        ratings_scores = ratings_scores.dropna()\n",
    "        cosine_scores = cosine_scores.drop(index_not_rated)\n",
    "        # print(f'--index_not_rated {index_not_rated} {len(cosine_scores)} {len(ratings_scores)}')\n",
    "        #calculating rating by weighted mean of ratings and cosine scores of the users who have rated the movie\n",
    "        ratings_movie = np.dot(ratings_scores, cosine_scores)/cosine_scores.sum()\n",
    "        return ratings_movie\n",
    "    \n",
    "    def fit(self, train_data, pre_fit: bool = False):\n",
    "        pass\n",
    "\n",
    "    def predict(self, data_at_test_timestamp, test_user, test_timestamp):\n",
    "        # print(f'--users list {test_user} {test_timestamp} {data_at_test_timestamp[\"Timestamp\"].max()} {np.sort(data_at_test_timestamp[\"UserID\"].unique())}')\n",
    "        ratings_by_user = data_at_test_timestamp.pivot(index='UserID', columns='MovieID', values='Rating')\n",
    "        df_ratings_dummy = ratings_by_user.copy().fillna(0)\n",
    "        similarity_matrix = cosine_similarity(df_ratings_dummy, df_ratings_dummy)\n",
    "        similarity_matrix_df = pd.DataFrame(similarity_matrix,\n",
    "                                            index=df_ratings_dummy.index,\n",
    "                                            columns=df_ratings_dummy.index)\n",
    "        items_candidates = data_at_test_timestamp['MovieID'].unique()\n",
    "        movies_already_watched = data_at_test_timestamp[\n",
    "            data_at_test_timestamp['UserID'] == test_user]['MovieID'].unique()\n",
    "        items_candidates = [item_id for item_id in items_candidates if item_id not in movies_already_watched]\n",
    "        collaborative_ratings = {}\n",
    "        for item_id in items_candidates:\n",
    "            collaborative_ratings[item_id] = self._weighted_avg_rating_for_movie(\n",
    "                ratings_by_user, similarity_matrix_df, item_id, test_user)\n",
    "        collaborative_ratings = pd.Series(collaborative_ratings)\n",
    "        # collaborative_ratings = (\n",
    "        #     collaborative_ratings/(collaborative_ratings.max() - collaborative_ratings.min()))*4 + 1\n",
    "        collaborative_ratings = collaborative_ratings.sort_values(\n",
    "            ascending=False) # kind='mergesort'\n",
    "        return collaborative_ratings.index.to_numpy(), collaborative_ratings.to_numpy()\n",
    "\n",
    "    def fit_predict(self, data, test_user, test_timestamp):\n",
    "        self.fit(data)\n",
    "        return self.predict(data, test_user, test_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv('../../data/ml-1m/ratings.dat',\n",
    "                         delimiter='::',\n",
    "                         header=None,\n",
    "                         names=['UserID','MovieID','Rating','Timestamp'],\n",
    "                         engine ='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the model for just a single point to view in what format the results are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_uu_model = CollaborativeFilteringUUModel()\n",
    "cf_uu_model.fit(eval_cf_uu.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids, predicted_ratings = cf_uu_model.predict(df_ratings[df_ratings['Timestamp'] < 978301619], 1, 978301619)\n",
    "# The rated movie is 2028, ground truth rating 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.31798032114613"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ratings[list(predicted_ids).index(2028)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And than let's do the evaluation. Also, for this model the both evaluation approaches are the same, as it has no training in the traditional sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in scalar divide\")\n",
    "# As there are a lot of such situations in the early time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cf_uu = EvaluationPipeline(df_ratings, 0.001) # .sample(frac=0.01, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2c9b4107b7439abae1dd901aa894b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_output_dict_cf_uu = eval_cf_uu.evaluate( # recommendation_results_baseline\n",
    "    cf_uu_model,\n",
    "    user_average_metrics=False,\n",
    "    retrain_model_each_point=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 2.2080150117431967,\n",
       " 'rmse': 2.6638655483638836,\n",
       " 'precision': 0.0,\n",
       " 'average_precision': 0.9974747474747475,\n",
       " 'mean_reciprocal_rank': 0.003836916995165856,\n",
       " 'ndcg': 0.9947807646755525,\n",
       " 'coverage': 0.013741721854304636}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_output_dict_cf_uu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cf_uu_no_updates_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_output_dict_cf_uu, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth noting, that the lower `MAE` and `RMSE` scores here than for the random train-test split without time dimension are explained by the fact that the similarity of the users can be measured with more accuracy the more data we have. But for the earlier users we can have a situation where a lot of the users who may be similar to them have still rated few movies, resulting in the early low-data prediction being worse. However, this is exasctly how the algorithm would work in the real application, as we would start from the less data and in process increase its amount and the quality of our predictions.\n",
    "\n",
    "Also, here the value of the average precision is by far the highest among all the reviewed algorithms so far. In our algorithm (refer to `src/evaluation.py`), average precision is calculated with the number `m` equal to the number of user's ratings in the test set. Comparing with the record low precision itself, this points out how the model is able to very accurately place the needed recommendation among the top points, but is not very good at rating it the highest. Still, the record-low so far values of `MAE` and `RMSE` point out that the model is good at predicting the movie's rating itself from similar users, so this model is a step in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
