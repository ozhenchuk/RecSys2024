{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41203dc7-fe1b-4fee-b927-fdd79e146b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "PROJECT_DIR = '../../'\n",
    "PROJECT_DIR = osp.abspath(PROJECT_DIR)\n",
    "print(PROJECT_DIR in sys.path)\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    print(f'Adding project directory to the sys.path: {PROJECT_DIR!r}')\n",
    "    sys.path.insert(1, PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86889395-693d-447a-9824-73a7213064e7",
   "metadata": {},
   "source": [
    "Let's construct our baseline model based on the items' mean rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e6c75a-754f-4b72-8f99-5d5ff41cd8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68c65d1-9c4b-4c2c-9a49-2cb046ad7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from tqdm.notebook import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de22a5fd-0987-4278-a7e9-020a76a3e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv('../../data/ml-1m/ratings.dat',\n",
    "                         delimiter='::',\n",
    "                         header=None,\n",
    "                         names=['UserID','MovieID','Rating','Timestamp'],\n",
    "                         engine ='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26ec1f2-33e2-43c1-96c1-990bafec64f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1193,  661,  914, 3408, 2355, 1197, 1287, 2804,  594,  919,  595,\n",
       "        938, 2398, 2918, 1035, 2791, 2687, 2018, 3105, 2797, 2321,  720,\n",
       "       1270,  527, 2340,   48, 1097, 1721, 1545,  745, 2294, 3186, 1566,\n",
       "        588, 1907,  783, 1836, 1022, 2762,  150,    1, 1961, 1962, 2692,\n",
       "        260, 1028, 1029, 1207, 2028,  531, 3114,  608, 1246])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings[df_ratings['UserID'] == 1]['MovieID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee77d124-ed17-472a-9c2d-11f5f977eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ratings = df_ratings.groupby('MovieID')['Rating'].mean()\n",
    "mean_ratings_candidates = mean_ratings[~mean_ratings.index.isin(\n",
    "    df_ratings[df_ratings['UserID'] == 1]['MovieID'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe969857-89f1-49f6-9e87-24e4e3c6f783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieID\n",
      "787     5.000000\n",
      "989     5.000000\n",
      "1830    5.000000\n",
      "3172    5.000000\n",
      "3233    5.000000\n",
      "3280    5.000000\n",
      "3382    5.000000\n",
      "3607    5.000000\n",
      "3656    5.000000\n",
      "3881    5.000000\n",
      "3245    4.800000\n",
      "53      4.750000\n",
      "2503    4.666667\n",
      "2905    4.608696\n",
      "2019    4.560510\n",
      "318     4.554558\n",
      "858     4.524966\n",
      "50      4.517106\n",
      "1148    4.507937\n",
      "439     4.500000\n",
      "Name: Rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(mean_ratings_candidates.sort_index().sort_values(kind='mergesort', ascending=False)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e88714f7-f286-48a4-98d2-9f6e13daf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractRSModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, train_data):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, data_at_test_timestamp, test_user, test_timestamp):\n",
    "        ...\n",
    "        # Returns: list of predicted items, list of their predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf212e6-349f-4159-83e6-0cb3896885c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineMeanRatingModel(AbstractRSModel):\n",
    "    def __init__(self):\n",
    "        self.pre_fit = False\n",
    "    \n",
    "    def fit(self, train_data, pre_fit: bool = False):\n",
    "        if self.pre_fit:\n",
    "            # The train data was already pre-fit\n",
    "            self.mean_ratings = train_data.groupby('MovieID')['Rating'].mean()\n",
    "        else:\n",
    "            self.mean_ratings = train_data.groupby('MovieID')['Rating'].mean()\n",
    "        self.pre_fit = pre_fit\n",
    "\n",
    "    def predict(self, data_at_test_timestamp, test_user, test_timestamp):\n",
    "        mean_ratings_candidates = self.mean_ratings[~self.mean_ratings.index.isin(\n",
    "            data_at_test_timestamp[data_at_test_timestamp['UserID'] == test_user]['MovieID'].unique())]\n",
    "        mean_ratings_candidates = mean_ratings_candidates.sort_values(ascending=False) # kind='mergesort'\n",
    "        return mean_ratings_candidates.index.to_numpy(), mean_ratings_candidates.to_numpy()\n",
    "\n",
    "    def fit_predict(self, data, test_user, test_timestamp):\n",
    "        self.fit(data)\n",
    "        return self.predict(data, test_user, test_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1901b338-e955-4767-8c1b-0c2e54491448",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_pred, ratings_pred = BaselineMeanRatingModel().fit_predict(df_ratings[\n",
    "                    df_ratings['Timestamp'] < 978301777], 1, 978301777) # 1028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcea471f-e086-461e-97ac-baba8d53e3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3881, 5.0), (3172, 5.0), (3280, 5.0), (787, 5.0), (3607, 5.0), (3522, 5.0), (3382, 5.0), (989, 5.0), (3233, 5.0), (3656, 5.0), (1830, 5.0), (578, 5.0), (53, 4.75), (2503, 4.666666666666667), (2930, 4.666666666666667), (2444, 4.666666666666667), (2905, 4.610169491525424), (2019, 4.572679509632224), (318, 4.558908045977011), (745, 4.52998379254457)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(items_pred[:20], ratings_pred[:20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c3e69-c527-4495-943a-02279982703b",
   "metadata": {},
   "source": [
    "Here we can see one of the disadvantages of a simple mean rating based recommender: it doesn't take into account the number of people who rated the movie. If some movie is rated 5.0 just once, it immediately moves on top of the list. Such a distadvantage can theoretically be fixed by adding weights to the item scores based on a movie popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "082d4295-2bcd-4e81-99de-36e5861dfc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import EvaluationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e19c361e-b96a-4e11-acc1-783692db928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserID               1\n",
      "MovieID           1193\n",
      "Rating               5\n",
      "Timestamp    978300760\n",
      "Name: 0, dtype: int64\n",
      "UserID               1\n",
      "MovieID            661\n",
      "Rating               3\n",
      "Timestamp    978302109\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i_p, test_point in df_ratings.iloc[:2].iterrows():\n",
    "    print(test_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "ae701d1a-937e-4d29-8fc3-a44263cf9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationPipeline:\n",
    "    def __init__(self,\n",
    "                 total_rating_data,\n",
    "                 train_test_split: float = 0.2):\n",
    "        self.total_rating_data = total_rating_data\n",
    "        self.train_test_split = train_test_split\n",
    "        self.test_data = self.get_test_data(self.total_rating_data)\n",
    "        self.train_data = self.total_rating_data[~self.total_rating_data.index.isin(self.test_data.index)]\n",
    "\n",
    "    def get_test_data(self, total_data):\n",
    "        return total_data.groupby('UserID', group_keys=False).apply(\n",
    "            lambda x: x.tail(int(np.round(x.shape[0]*self.train_test_split)))).sort_values('Timestamp')\n",
    "\n",
    "    def evaluate(self,\n",
    "                 model_object,\n",
    "                 metrics_list = False,\n",
    "                 user_average_metrics: bool = False,\n",
    "                 retrain_model_each_point: bool = False):\n",
    "        if not metrics_list:\n",
    "            metrics_list = ['mae']\n",
    "        if user_average_metrics:\n",
    "            recommendation_results = {}\n",
    "        else:\n",
    "            recommendation_results = []\n",
    "        total_unique_timestamps = np.sort(self.total_rating_data['Timestamp'].unique()).tolist()\n",
    "        total_data_split = {timestamp : self.total_rating_data[self.total_rating_data[\n",
    "                            'Timestamp'] == timestamp] for timestamp in total_unique_timestamps}\n",
    "        test_timestamp_min = self.test_data['Timestamp'].min()\n",
    "        # timestamps_included_test_each_point = total_unique_timestamps[\n",
    "        #     total_unique_timestamps < test_timestamp_min]\n",
    "        latest_included_timestamp_test_each_point = total_unique_timestamps.index(test_timestamp_min) - 1\n",
    "        test_data_each_point = self.total_rating_data[\n",
    "                self.total_rating_data['Timestamp'] < test_timestamp_min]\n",
    "        previous_timestamp = test_timestamp_min\n",
    "        for i_p, test_point in tqdm(self.test_data.iterrows(),\n",
    "                                    total=self.test_data.shape[0]):\n",
    "            test_point_timestamp = test_point['Timestamp']\n",
    "            if test_point_timestamp > previous_timestamp:\n",
    "                timestamps_to_include = total_unique_timestamps[latest_included_timestamp_test_each_point+1:total_unique_timestamps.index(test_point_timestamp)]\n",
    "                for timestamp_include in timestamps_to_include:\n",
    "                    test_data_each_point = pd.concatenate([\n",
    "                        test_data_each_point,\n",
    "                        total_data_split[timestamp_include]\n",
    "                    ])\n",
    "                latest_included_timestamp_test_each_point = total_unique_timestamps.index(test_point_timestamp)\n",
    "                previous_timestamp = test_point_timestamp\n",
    "            test_point_user = test_point['UserID']\n",
    "            if user_average_metrics and test_point_user not in recommendation_results.keys():\n",
    "                recommendation_results[test_point_user] = []\n",
    "            if retrain_model_each_point:\n",
    "                train_data_each_point = self.train_data[\n",
    "                    self.train_data['Timestamp'] < test_point_timestamp]\n",
    "                model_object.fit(train_data_each_point)\n",
    "            # Using the whole dataset, as for each new test point for the user previous test points matter too\n",
    "            items_pred, ratings_pred = model_object.predict(test_data_each_point,\n",
    "                                                            test_point_user,\n",
    "                                                            test_point_timestamp)\n",
    "            if user_average_metrics:\n",
    "                recommendation_results[test_point_user].append([self.test_data.index[i_p], items_pred, ratings_pred])\n",
    "            else:\n",
    "                recommendation_results.append([self.test_data.index[i_p], items_pred, ratings_pred])\n",
    "        metrics_output_dict = {}\n",
    "        if not user_average_metrics:\n",
    "            ratings_y_true = [self.test_data.loc[\n",
    "                              test_point_index, 'Rating'] for test_point_index in self.test_data.index]\n",
    "            ratings_y_pred = [ratings_pred[list(items_pred).index(\n",
    "                self.test_data.loc[test_point_index, 'MovieID'])] if self.test_data.loc[\n",
    "                    test_point_index, 'MovieID'] in items_pred else 0 for test_point_index in self.test_data.index]\n",
    "            for metric in metrics_list:\n",
    "                if metric == 'mae':\n",
    "                    metrics_output_dict['mae'] = mean_absolute_error(ratings_y_true, ratings_y_pred)\n",
    "        return recommendation_results, metrics_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "0352bf0b-06cf-420f-86d9-e92578ea4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "dfc1f03c-5d1a-459b-96d6-dba3761c0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "37d1a649-f1c3-4d2e-b6e5-42019989d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationPipeline:\n",
    "    def __init__(self,\n",
    "                 total_rating_data,\n",
    "                 train_test_split: float = 0.2):\n",
    "        self.total_rating_data = total_rating_data\n",
    "        self.train_test_split = train_test_split\n",
    "        self.test_data = self.get_test_data(self.total_rating_data)\n",
    "        self.train_data = self.total_rating_data[~self.total_rating_data.index.isin(self.test_data.index)]\n",
    "\n",
    "    def get_test_data(self, total_data):\n",
    "        return total_data.groupby('UserID', group_keys=False).apply(\n",
    "            lambda x: x.tail(int(np.round(x.shape[0]*self.train_test_split))))\n",
    "\n",
    "    def evaluate(self,\n",
    "                 model_object,\n",
    "                 metrics_list = False,\n",
    "                 user_average_metrics: bool = False,\n",
    "                 retrain_model_each_point: bool = False):\n",
    "        if not metrics_list:\n",
    "            metrics_list = ['mae']\n",
    "        if user_average_metrics:\n",
    "            recommendation_results = {}\n",
    "        else:\n",
    "            recommendation_results = []\n",
    "        pool_parameters = [(self.train_data,\n",
    "                            self.total_rating_data,\n",
    "                            i_p,\n",
    "                            model_object,\n",
    "                            test_point,\n",
    "                            user_average_metrics,\n",
    "                            retrain_model_each_point) for i_p, test_point in tqdm(self.test_data.iterrows(),\n",
    "                                                                                  total=self.test_data.shape[0])]\n",
    "        with ThreadPool(processes=8) as pool:\n",
    "            recommendation_results = list(tqdm(Pool(8).imap(_predict_for_point, pool_parameters[:1000]),\n",
    "                total=len(pool_parameters[:1000])))\n",
    "        # for i_p, test_point in tqdm(self.test_data.iterrows(),\n",
    "        #                             total=self.test_data.shape[0]):\n",
    "        #     recommendation_results.append(self._predict_for_point(model_object,\n",
    "        #         test_point,\n",
    "        #         user_average_metrics=user_average_metrics,\n",
    "        #         retrain_model_each_point=retrain_model_each_point))\n",
    "        metrics_output_dict = {}\n",
    "        if not user_average_metrics:\n",
    "            ratings_y_true = [self.test_data.loc[\n",
    "                              test_point_index, 'Rating'] for test_point_index in self.test_data.index]\n",
    "            ratings_y_pred = [ratings_pred[list(items_pred).index(\n",
    "                self.test_data.loc[test_point_index, 'MovieID'])] if self.test_data.loc[\n",
    "                    test_point_index, 'MovieID'] in items_pred else 0 for test_point_index in self.test_data.index]\n",
    "            for metric in metrics_list:\n",
    "                if metric == 'mae':\n",
    "                    metrics_output_dict['mae'] = mean_absolute_error(ratings_y_true, ratings_y_pred)\n",
    "        return recommendation_results, metrics_output_dict\n",
    "\n",
    "def _predict_for_point(args):\n",
    "    # train_data = args[0]\n",
    "    # total_rating_data = args[1]\n",
    "    # test_data_index = args[2]\n",
    "    # model_object = args[3]\n",
    "    test_point = args[4]\n",
    "    user_average_metrics = args[5]\n",
    "    retrain_model_each_point = args[6]\n",
    "    test_point_timestamp = test_point['Timestamp']\n",
    "    test_point_user = test_point['UserID']\n",
    "    if user_average_metrics and test_point_user not in recommendation_results.keys():\n",
    "        recommendation_results[test_point_user] = []\n",
    "    if retrain_model_each_point:\n",
    "        train_data_each_point = args[0][\n",
    "            args[0]['Timestamp'] < test_point_timestamp]\n",
    "        model_object.fit(train_data_each_point)\n",
    "    # Using the whole dataset, as for each new test point for the user previous test points matter too\n",
    "    test_data_each_point = args[1][\n",
    "            args[1]['Timestamp'] < test_point_timestamp]\n",
    "    items_pred, ratings_pred = args[3].predict(test_data_each_point,\n",
    "                                                    test_point_user,\n",
    "                                                    test_point_timestamp)\n",
    "    if user_average_metrics:\n",
    "        return [args[2], items_pred, ratings_pred, test_point_user]\n",
    "    else:\n",
    "        return [args[2], items_pred, ratings_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fd8965e9-cae1-41bc-915e-8d8165ace795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp\n",
       "956703932           1\n",
       "956703954           3\n",
       "956703977           5\n",
       "956704056          10\n",
       "956704081          11\n",
       "               ...   \n",
       "1046454320    1000204\n",
       "1046454338    1000205\n",
       "1046454443    1000206\n",
       "1046454548    1000208\n",
       "1046454590    1000209\n",
       "Name: UserID, Length: 458455, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.sort_values('Timestamp').groupby('Timestamp').count()['UserID'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b68e5a3f-965a-4d35-927d-d4bed1fa1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f435f7f-9084-4d1b-aed0-aa12cd0faa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd28ce88-300e-414a-b69a-fa945f3f2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a5a93d7-f711-4a10-8178-4dbfd965b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_special(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred)/len(y_true)\n",
    "\n",
    "def precision_top_k(y_true, y_pred, k):\n",
    "    return np.sum(y_true[:k] == y_pred[:k])/k\n",
    "\n",
    "def average_precision(y_true, y_pred, m):\n",
    "    return np.sum([precision_top_k(y_true, y_pred, k) for k in range(1,m+1)])/m\n",
    "\n",
    "def argsort_top_n(y_list, n):\n",
    "    indices_unsorted = np.argpartition(y_list, -n)[-n:]\n",
    "    combined_time_order_unsorted = np.array([np.array(y_list)[indices_unsorted], -indices_unsorted]).T\n",
    "    return indices_unsorted[np.lexsort(combined_time_order_unsorted[:,::-1].T)][::-1]\n",
    "\n",
    "def mean_reciprocal_rank(relevant_items_list, predicted_candidates_lists):\n",
    "    ranks = [list(predicted_candidates_lists[i_i]).index(\n",
    "        item)+1 if item in predicted_candidates_lists[i_i] else 0 for i_i, item in enumerate(relevant_items_list)]\n",
    "    return np.sum([1/rank if rank > 0 else 0 for rank in ranks])/len(relevant_items_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb30b8ff-76ff-4561-a91d-25a90af81937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationPipeline:\n",
    "    def __init__(self,\n",
    "                 total_rating_data,\n",
    "                 train_test_split: float = 0.2):\n",
    "        self.total_rating_data = total_rating_data\n",
    "        self.train_test_split = train_test_split\n",
    "        self.test_data = self.get_test_data(self.total_rating_data)\n",
    "        self.train_data = self.total_rating_data[~self.total_rating_data.index.isin(self.test_data.index)]\n",
    "\n",
    "    def get_test_data(self, total_data):\n",
    "        return total_data.groupby('UserID', group_keys=False).apply(\n",
    "            lambda x: x.tail(int(np.round(x.shape[0]*self.train_test_split))))\n",
    "\n",
    "    def evaluate(self,\n",
    "                 model_object,\n",
    "                 metrics_list = False,\n",
    "                 user_average_metrics: bool = False,\n",
    "                 retrain_model_each_point: bool = False):\n",
    "        if not metrics_list:\n",
    "            metrics_list = ['mae','rmse','precision','average_precision',\n",
    "                            'mean_reciprocal_rank','ndcg','coverage']\n",
    "        if user_average_metrics:\n",
    "            recommendation_results = {}\n",
    "            ratings_y_pred = {}\n",
    "        else:\n",
    "            recommendation_results = []\n",
    "            ratings_y_pred = []\n",
    "        sorted_total_data_test = self.total_rating_data.sort_values('Timestamp')\n",
    "        rows_before_timestamp_test = sorted_total_data_test.groupby('Timestamp').count()[\n",
    "            'UserID'].cumsum().shift(1).fillna(0).astype(int).to_dict()\n",
    "        if retrain_model_each_point:\n",
    "            sorted_data_train = self.train_data.sort_values('Timestamp')\n",
    "            rows_before_timestamp_train = sorted_data_train.groupby('Timestamp').count()[\n",
    "                'UserID'].cumsum().shift(1).fillna(0).astype(int)\n",
    "            timestamps_not_in_train = [timestamp for timestamp in rows_before_timestamp_test.keys(\n",
    "                ) if timestamp not in rows_before_timestamp_train.index]\n",
    "            rows_before_timestamp_train = pd.concat([rows_before_timestamp_train,\n",
    "                                                     pd.Series(None,\n",
    "                                                               index=timestamps_not_in_train)])\n",
    "            rows_before_timestamp_train = rows_before_timestamp_train.sort_index().ffill().astype(int)\n",
    "        for i_p, test_point in tqdm(self.test_data.iterrows(),\n",
    "                                    total=self.test_data.shape[0]):\n",
    "            test_point_timestamp = test_point['Timestamp']\n",
    "            test_point_user = test_point['UserID']\n",
    "            if user_average_metrics and test_point_user not in recommendation_results.keys():\n",
    "                recommendation_results[test_point_user] = []\n",
    "                ratings_y_pred[test_point_user] = []\n",
    "            if retrain_model_each_point:\n",
    "                train_data_each_point = sorted_data_train[:rows_before_timestamp_train[test_point_timestamp]]\n",
    "                model_object.fit(train_data_each_point)\n",
    "            # Using the whole dataset, as for each new test point for the user previous test points matter too\n",
    "            items_pred, ratings_pred = model_object.predict(sorted_total_data_test[\n",
    "                                                                :rows_before_timestamp_test[test_point_timestamp]],\n",
    "                                                            test_point_user,\n",
    "                                                            test_point_timestamp)\n",
    "            if user_average_metrics:\n",
    "                recommendation_results[test_point_user].append([i_p, items_pred])\n",
    "                ratings_y_pred[test_point_user].append(ratings_pred[items_pred.tolist().index(\n",
    "                    test_point_user)] if (test_point_user in items_pred) else 0)\n",
    "            else:\n",
    "                recommendation_results.append([i_p, items_pred])\n",
    "                ratings_y_pred.append(ratings_pred[items_pred.tolist().index(\n",
    "                    test_point_user)] if (test_point_user in items_pred) else 0)\n",
    "        metrics_output_dict = {}\n",
    "        if not user_average_metrics:\n",
    "            ratings_y_true = [self.test_data.loc[\n",
    "                              test_point_index, 'Rating'] for test_point_index in self.test_data.index]\n",
    "            # ratings_y_pred = [ratings_pred[items_pred.index(\n",
    "            #     self.test_data.loc[test_point_index, 'movieID'])] if self.test_data.loc[\n",
    "            #         test_point_index, 'movieID'] in items_pred else 0 for test_point_index in self.test_data.index]\n",
    "            # ratings_y_pred = [recommendation_results[i_p][2][recommendation_results[i_p][1].tolist().index(\n",
    "            #     self.test_data.loc[test_point_index, 'MovieID'])] if self.test_data.loc[\n",
    "            #         test_point_index, 'MovieID'] in recommendation_results[i_p][\n",
    "            #             1] else 0 for i_p, test_point_index in enumerate(self.test_data.index)]\n",
    "            self.test_data['Rating pred'] = ratings_y_pred\n",
    "            ratings_y_true_users = self.test_data.groupby('UserID')['Rating'].apply(list).to_dict()\n",
    "            ratings_y_pred_users = self.test_data.groupby('UserID')['Rating pred'].apply(list).to_dict()\n",
    "            largest_user_id_total = self.total_rating_data['UserID'].max()\n",
    "            items_id_pred = [pred[1][0] if len(pred[1]) > 0 else (\n",
    "                largest_user_id_total + 1) for pred in recommendation_results]\n",
    "            for metric in metrics_list:\n",
    "                if metric == 'mae':\n",
    "                    metrics_output_dict['mae'] = mean_absolute_error(ratings_y_true, ratings_y_pred)\n",
    "                elif metric == 'rmse':\n",
    "                    metrics_output_dict['rmse'] = sqrt(mean_squared_error(ratings_y_true, ratings_y_pred))\n",
    "                elif metric == 'precision':\n",
    "                    metrics_output_dict['precision'] = precision_special(self.test_data['MovieID'].to_numpy(),\n",
    "                                                                         items_id_pred)\n",
    "                elif metric == 'average_precision':\n",
    "                    average_precision_list = []\n",
    "                    for user in self.test_data['UserID'].unique():\n",
    "                        m_user = len(ratings_y_true_users[user])\n",
    "                        average_precision_list.append(average_precision(\n",
    "                            argsort_top_n(ratings_y_true_users[user], m_user),\n",
    "                            argsort_top_n(ratings_y_pred_users[user], m_user),\n",
    "                            m_user))\n",
    "                    metrics_output_dict['average_precision'] = np.mean(average_precision_list)\n",
    "                elif metric == 'mean_reciprocal_rank':\n",
    "                    metrics_output_dict['mean_reciprocal_rank'] = mean_reciprocal_rank(\n",
    "                        self.test_data['MovieID'].to_numpy(),\n",
    "                        [pred[1] for pred in recommendation_results])\n",
    "                elif metric == 'ndcg':\n",
    "                    ndcg = []\n",
    "                    for user in self.test_data['UserID'].unique():\n",
    "                        m_user = len(ratings_y_true_users[user])\n",
    "                        if m_user > 1:\n",
    "                            # NDCG only is defined is there is more than 1 point\n",
    "                            ndcg.append(ndcg_score(\n",
    "                                np.array(ratings_y_true_users[user])[\n",
    "                                    argsort_top_n(ratings_y_true_users[user], m_user)].reshape(1,-1),\n",
    "                                np.array(ratings_y_pred_users[user])[\n",
    "                                    argsort_top_n(ratings_y_pred_users[user], m_user)].reshape(1,-1)))\n",
    "                    if len(ndcg) > 0:\n",
    "                        metrics_output_dict['ndcg'] = np.mean(ndcg)\n",
    "                    else:\n",
    "                        metrics_output_dict['ndcg'] = 0.0\n",
    "                elif metric == 'coverage':\n",
    "                    items_train_unique = self.train_data['UserID'].unique()\n",
    "                    metrics_output_dict['coverage'] = len(np.unique([\n",
    "                        item for item in items_id_pred if item in items_train_unique]))/len(items_train_unique)\n",
    "        return metrics_output_dict # recommendation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7406b6-ca05-4e7e-af7a-efa1a0a84fd6",
   "metadata": {},
   "source": [
    "Let's evaluate the model with two variants of evaluation algorithms. They differ on the account how much data is visible to the model at the prediction time of each point and whether it was able to correct its predictions with the data that came after the training data last point, but before this point.\n",
    "\n",
    "The **first approach** (called *time cut-off* in the evaluation experiment 1.5) performs model retraining/updates for each point, and is a full correct version of the evaluation pipeline that uses up-to-date version of the model. This approach exactly emulates how our model would've worked if it was launched at the time of the recommendation point. We will primarily use this variant further.\n",
    "\n",
    "The **second approach** (*static*) is without after-training model tuning and in it the model essentially doesn't update itself based on the new information from the data points that appeared in test before this point. The data for prediction still obviously counts all the points before this, but the model completely relies on the train data. therefore, this approach should make the model less accurate, but more stable and the same for all recommendations.\n",
    "\n",
    "The need for this mode is caused by some future approaches (like deep learning-based) not having a reasonable possibility to be fully retrained for each new time point. After all, the real-life model is also unlikely to retrain every few seconds, and would insted perform un updates on some schedule or by observing relevant for the system metrics. Therefore, this evaluation model is a reasonable compromise between the almost mathematically ideal approach (first approach) and real-life computational and evaluational constraints.\n",
    "\n",
    "Where it is possible, we will perform both evaluation approaches to the model for a better comparison. However, they will also help us to understand how important is model online retraining for each model.\n",
    "\n",
    "Therefore, the baseline mean-rating model evaluated with updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fd00d622-480d-456b-a96d-7c14bdc7de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a29786b7-fb17-4cb9-8bea-3f30ae9a017a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01679706573486328"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "sum_values = np.zeros(df_ratings['MovieID'].max()+1)\n",
    "values = df_ratings[['MovieID','Rating']].values.T\n",
    "sum_values = np.bincount(values[0], weights=values[1])\n",
    "sum_values = np.bincount(values[0], weights=np.ones(len(values[0])).astype(int))\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "850161d2-2b61-41f9-b818-06157e49526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1193,  661,  914, ...,  562, 1096, 1097],\n",
       "       [   5,    3,    3, ...,    5,    4,    4]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "48833fe3-9928-43a6-b7a0-e2a75eba83c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 1.8659298e+09])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d6f2673e-6c9b-485f-90f0-55f97e2ffe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MovieID\n",
       "1       4.146846\n",
       "2       3.201141\n",
       "3       3.016736\n",
       "4       2.729412\n",
       "5       3.006757\n",
       "          ...   \n",
       "3948    3.635731\n",
       "3949    4.115132\n",
       "3950    3.666667\n",
       "3951    3.900000\n",
       "3952    3.780928\n",
       "Name: Rating, Length: 3706, dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cbfdb568-9a64-4dc7-8bc0-cf5692934364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02746105194091797"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "mean_values = df_ratings[['MovieID','Rating']].groupby('MovieID')['Rating'].mean()\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cd93adff-2931-41d8-be23-824e92246407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MovieID\n",
       "1       4.146846\n",
       "2       3.201141\n",
       "3       3.016736\n",
       "4       2.729412\n",
       "5       3.006757\n",
       "          ...   \n",
       "3948    3.635731\n",
       "3949    4.115132\n",
       "3950    3.666667\n",
       "3951    3.900000\n",
       "3952    3.780928\n",
       "Name: Rating, Length: 3706, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8a33c70-a660-48cc-8d0c-1500f64ed4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineMeanRatingModel(AbstractRSModel):\n",
    "    def __init__(self):\n",
    "        self.pre_fit = False\n",
    "    \n",
    "    def fit(self, train_data, pre_fit: bool = False):\n",
    "        if self.pre_fit:\n",
    "            # The train data was already pre-fit\n",
    "            self.mean_ratings = train_data[['MovieID','Rating']].groupby('MovieID')['Rating'].mean()\n",
    "        else:\n",
    "            self.mean_ratings = train_data[['MovieID','Rating']].groupby('MovieID')['Rating'].mean()\n",
    "        self.pre_fit = pre_fit\n",
    "\n",
    "    def predict(self, data_at_test_timestamp, test_user, test_timestamp):\n",
    "        mean_ratings_candidates = self.mean_ratings[~self.mean_ratings.index.isin(\n",
    "            data_at_test_timestamp[data_at_test_timestamp['UserID'] == test_user]['MovieID'].unique())]\n",
    "        mean_ratings_candidates = mean_ratings_candidates.sort_values(ascending=False) # kind='mergesort'\n",
    "        return mean_ratings_candidates.index.to_numpy(), mean_ratings_candidates.to_numpy()\n",
    "\n",
    "    def fit_predict(self, data, test_user, test_timestamp):\n",
    "        self.fit(data)\n",
    "        return self.predict(data, test_user, test_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e847fc3-be51-4405-85c8-0fd1655f86de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25162/456114600.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return total_data.groupby('UserID', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "eval_baseline = EvaluationPipeline(df_ratings, 0.2) # .sample(frac=0.01, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "132d965e-09e5-4e80-8add-fbb49760422c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11143e79255423a9d7637f419afee97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200016 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_output_dict_baseline = eval_baseline.evaluate( # recommendation_results_baseline\n",
    "    BaselineMeanRatingModel(),\n",
    "    user_average_metrics=False,\n",
    "    retrain_model_each_point=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fa4fc7a-501d-4f96-809a-5cc5b50703a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 2.27333603784064,\n",
       " 'rmse': 2.7554581166663943,\n",
       " 'precision': 0.000499960003199744,\n",
       " 'average_precision': 0.133991571427397,\n",
       " 'mean_reciprocal_rank': 0.004113712180765098,\n",
       " 'ndcg': 0.9363506966248356,\n",
       " 'coverage': 0.06986754966887417}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_output_dict_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d89183a-94cc-44cb-bd4f-926f391e5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baseline_mean_rating_with_updates_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_output_dict_baseline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5e2c5-191a-400f-8ea4-9070672802f8",
   "metadata": {},
   "source": [
    "Here we can see that the main metrics are quite low, as can be predicted of the baseline model with no training or user and item contexts. MAE indicates that each our rank prediction had an error of 2.27 rating points on average, which is still a pretty high number given that our ratings are in the range of 1-5 with the majority of the average ratings of items being in the range of according to the EDA. One exception to this is the ranking-based `NDCG`, which we will address after we will evaluate the second approach.\n",
    "\n",
    "Another notable thing is that coverage is quite good comparing of what would be expected of the baseline no-learning recommender (almost 7% of the total training points were recommended at some point!). The resason for that is of curse the nature of the evaluation algorithm, as it evaluatesthe model exactly as it would've been at each historical point. As the new data arrives, the average ratings shift, and the resulting recommendations change in time.\n",
    "\n",
    "Then, the baseline mean-rating model evaluated without updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f978d51-e098-4016-bfe3-f9a90869c5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25162/456114600.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return total_data.groupby('UserID', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "eval_baseline_no_updates = EvaluationPipeline(df_ratings, 0.2) # .sample(frac=0.01, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a2795e1-a716-41df-a2a4-9b44f0d18fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_nodel_not_each_point = BaselineMeanRatingModel()\n",
    "baseline_nodel_not_each_point.fit(eval_baseline_no_updates.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd747107-9424-44eb-8d8a-37bda2f53b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df85cbd046df46fea0042ab5cd61e45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200016 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_output_dict_baseline_no_updates = eval_baseline_no_updates.evaluate( # recommendation_results_baseline_no_updates\n",
    "    baseline_nodel_not_each_point,\n",
    "    user_average_metrics=False,\n",
    "    retrain_model_each_point=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16b02f03-4771-4ee7-975e-53ab6e9c3657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 2.179565309324234,\n",
       " 'rmse': 2.6676582273708953,\n",
       " 'precision': 0.00021498280137588992,\n",
       " 'average_precision': 0.13829390681488837,\n",
       " 'mean_reciprocal_rank': 0.004104411391279518,\n",
       " 'ndcg': 0.9273651300033005,\n",
       " 'coverage': 0.002814569536423841}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_output_dict_baseline_no_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "198a0b05-c5a5-4d3d-80b1-669396256972",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baseline_mean_rating_no_updates_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_output_dict_baseline_no_updates, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218da68-73f7-4c52-bc7a-4221dab21af0",
   "metadata": {},
   "source": [
    "And, as we can see, our metrics indeed mostly show that the baseline mean rating model with updates is better, as it takes new points before the prediction into account, and the no-update model just trains on the static training data. This is specifically visible on the `coverage` metric, which here is much worse due to the evaluation on the static data. This highlights to us that the evaluation with updates is indeed the more optimal strategy, which on each step takes into account all information available about the data up to that point.\n",
    "\n",
    "However, for both of the baseline metrics the values of the most metrics are low. This is obviously due to the fact that our model didn't really learn anything and didn't even take into account the fact that some of the movies with theoretically the best rating (often 5.0) have just several reviewes, and thus the predictions with this model are not very good. This can partially be fixed with the popularity-based baseline model, which we will try next as a bonus and because of the interest in whether it will indeed improve the predictions for the users who keep to the mainstream films.\n",
    "\n",
    "Also, the model that had no updates had just slightly better `MAE` and `RMSE` values, highlighting that the simple average may approximate the users' average just a little closer, but the change is so small that it may not be significant and just be caused by the changes in the averages that have no connection with the selected user's ratings. For further conclusions, we would need to compare these results with the other models to get the good reference points for `MAE` and `RMSE` metrics.\n",
    "\n",
    "From other interesting points gathered from the no update evaluation, some ranking scores like `NDCG` produce noticeably inaccurate results for our dataset. This is unlikely to change for the better models, as our ground truth movies rankings rely on their user ratings, which have just 5 possible values. This makes ranking and ordering the offline ground truth candidates difficult if the user is very active and has many 5.0-rated movies in his test split.\n",
    "\n",
    "Obviously, this will change if we have an online validation, as there A/B testing and hit rate-based approaches should take over the model's evaluation and significantly improve the users' feedback on the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c74999-82d2-43aa-a8f6-ad1ce6552d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588204a8-fd9c-4d82-a93a-f84bca2a9665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
