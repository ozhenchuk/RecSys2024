{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590c2280-d462-469b-a377-90c45ac7b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "PROJECT_DIR = '../../'\n",
    "PROJECT_DIR = osp.abspath(PROJECT_DIR)\n",
    "print(PROJECT_DIR in sys.path)\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    print(f'Adding project directory to the sys.path: {PROJECT_DIR!r}')\n",
    "    sys.path.insert(1, PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234048ab-bcda-41dc-9aa8-86529e920b08",
   "metadata": {},
   "source": [
    "Let's construct our baseline model based on the items' popularity.\n",
    "\n",
    "For this system we will need a way to represent our popularity ratings in a way that will theoretically predict the ground truth user rankings. As such, the calculated popularity scores will be normalized to the range [1.0;5.0], where 1 will be the item that not a single user has rated yet, and 5.0 will be the item with the most ratings at the given time point.\n",
    "\n",
    "The evaluation pipeline here will follow the same two approaches as for the previous mean-rating baseline experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae922318-b3ae-47d1-8e94-4bb0a3f23405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.abstract_rs_model import AbstractRSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57288a13-2dde-47b6-b99f-53717f02e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from tqdm.notebook import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab542209-763b-4549-ae91-3a7f6549d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv('../../data/ml-1m/ratings.dat',\n",
    "                         delimiter='::',\n",
    "                         header=None,\n",
    "                         names=['UserID','MovieID','Rating','Timestamp'],\n",
    "                         engine ='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81462556-a31c-4cb7-bb16-9271407ee589",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ratings = df_ratings.groupby('MovieID')['Rating'].count()\n",
    "count_ratings_candidates = count_ratings[~count_ratings.index.isin(\n",
    "    df_ratings[df_ratings['UserID'] == 0]['MovieID'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b05ea5f-bd62-4dd7-83dc-4ea65332f55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MovieID\n",
       "2858    3428\n",
       "260     2991\n",
       "1196    2990\n",
       "1210    2883\n",
       "480     2672\n",
       "        ... \n",
       "3237       1\n",
       "763        1\n",
       "624        1\n",
       "2563       1\n",
       "3290       1\n",
       "Name: Rating, Length: 3706, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_ratings_candidates.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "909cf863-fbcb-4fd8-ab68-d37287d17211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import EvaluationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8be981e-9279-464d-ae0d-db7f33b5bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselinePopularityModel(AbstractRSModel):\n",
    "    def __init__(self):\n",
    "        self.pre_fit = False\n",
    "    \n",
    "    def fit(self, train_data, pre_fit: bool = False):\n",
    "        if self.pre_fit:\n",
    "            # The train data was already pre-fit\n",
    "            self.count_ratings = train_data[['MovieID','Rating']].groupby('MovieID')['Rating'].count()\n",
    "        else:\n",
    "            self.count_ratings = train_data[['MovieID','Rating']].groupby('MovieID')['Rating'].count()\n",
    "        self.count_ratings = (self.count_ratings/self.count_ratings.max())*4 + 1\n",
    "        self.pre_fit = pre_fit\n",
    "\n",
    "    def predict(self, data_at_test_timestamp, test_user, test_timestamp):\n",
    "        mean_ratings_candidates = self.count_ratings[~self.count_ratings.index.isin(\n",
    "            data_at_test_timestamp[data_at_test_timestamp['UserID'] == test_user]['MovieID'].unique())]\n",
    "        mean_ratings_candidates = mean_ratings_candidates.sort_values(ascending=False) # kind='mergesort'\n",
    "        return mean_ratings_candidates.index.to_numpy(), mean_ratings_candidates.to_numpy()\n",
    "\n",
    "    def fit_predict(self, data, test_user, test_timestamp):\n",
    "        self.fit(data)\n",
    "        return self.predict(data, test_user, test_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8a8a8b3-fa2e-4068-a03d-4b51a92025d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_pred, ratings_pred = BaselinePopularityModel().fit_predict(df_ratings[\n",
    "                    df_ratings['Timestamp'] < 978301777], 1, 978301777) # 1028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc1546c0-8fd8-44cb-9036-7ce2b2fd195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2858, 5.0), (1196, 4.45913096323306), (1210, 4.36918869644485), (480, 4.110300820419326), (589, 4.095715587967183), (2571, 3.972956548161653), (1580, 3.9571558796718325), (593, 3.955940443634154), (1198, 3.8903068975995136), (110, 3.839258584017016), (2762, 3.7675478577939834), (2396, 3.728653904588271), (1197, 3.672743846855059), (527, 3.65572774232756), (1617, 3.6241264053479187), (1097, 3.587663324217563), (1265, 3.5779398359161347), (2997, 3.568216347614707), (2628, 3.5670009115770283), (318, 3.5378304466727437)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(items_pred[:20], ratings_pred[:20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863d105-0b40-4b8c-8fe4-0c62487f7b3f",
   "metadata": {},
   "source": [
    "As we can see, the predictions for a certain point are slightly different that the ones built on the whole dataset: the number of ratings changes with time, and so are our predictions, which should be relevant for the exact time point we make the recommendations on. Therefore, the evaluation approach with the model being updated as much as possible for the each time point is the best here, as for the mean-rating baseline model.\n",
    "\n",
    "There are also some advantages of this model comared to the mean-rating one visible even here. For example, we no longer have a situation where a single user rating a movie 5.0 has an immediate large effect on the predictions. Here all of the values are more spread out in the distribution, which is further improved with normalization.\n",
    "\n",
    "However, it is obviously done with a tradeoff that the predicted ratings have nothing to do with the estimated movie quality or the other user's opinion for it. We may recommend a very low-rated movie that many people just happen to watch. But there is also a possibility that the target user may also want to watch the movie anyway, as it is very popular. All of this is very far from having any insight on the user itself, though.\n",
    "\n",
    "So, let's start with the first evaluation approach where the model updating is carried out for each new predicting time point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b12f539-954c-475b-9411-8317778c1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_baseline = EvaluationPipeline(df_ratings, 0.2) # .sample(frac=0.01, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2baf339-5683-44df-8090-618116be927d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a5c1d323604bb2b812bae8c5e26e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200016 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_output_dict_baseline = eval_baseline.evaluate( # recommendation_results_baseline\n",
    "    BaselinePopularityModel(),\n",
    "    user_average_metrics=False,\n",
    "    retrain_model_each_point=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0587f169-d079-474a-a316-5e1bc32b6687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 2.9840492508644663,\n",
       " 'rmse': 3.230121187663979,\n",
       " 'precision': 0.0004649628029757619,\n",
       " 'average_precision': 0.12319586042903112,\n",
       " 'mean_reciprocal_rank': 0.003951362090124058,\n",
       " 'ndcg': 0.9513780267109001,\n",
       " 'coverage': 0.012417218543046357}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_output_dict_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "007c788d-bc27-4dba-b6c7-81e4826bb94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baseline_popularity_with_updates_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_output_dict_baseline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a0088-09f4-407e-b26f-75cd40ba12be",
   "metadata": {},
   "source": [
    "As we can see, here almost all of the metrics are worse than for a similar evaluation for the mean-rating baseline model. The most noticeable change is `RMSE` - it has increased significantly, but, more importantly, is now closer to `MAE` than it was for the mean-rating model. This indicates that there are less rating predictions in our data that are very far from the ground truth ratings by the users and result in large errors, or, rather, than the errors are generally more similar in absolute values. Therefore, the mean-rating baseline produced large errors because the mean ratings of other users are sometimes predictive for the target user, but sometimes he has other opinion and the prediction is largely inaccurate. In the case of the popularity-based model however, the number of ratings is less predictive overall of the target user's rating, so there is less situations where the baseline model was indeed close to the target. From this we can conclude that popularity indeed cannot be a good approximation of the rating, but it geves more uniform results.\n",
    "\n",
    "The same can be said about the ranking-based metrics, though `NDCG` continues to produce largely inaccurate results due to the mensioned preperties of the dataset (low discretization of the ground truth values with only 4 rating options available for the users).\n",
    "\n",
    "Now, let's evaluate the baseline popularity-based model with the evaluation approach without updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abf231af-ec7c-4ecd-907c-8285f9d82a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_baseline_no_updates = EvaluationPipeline(df_ratings, 0.2) # .sample(frac=0.01, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6708029a-1715-4a48-949c-4ecbf8cd0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_nodel_not_each_point = BaselinePopularityModel()\n",
    "baseline_nodel_not_each_point.fit(eval_baseline_no_updates.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb867158-96fd-4986-a935-c6584ca0055a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61eed66fa03743b1af1694eb95ae52bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200016 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_output_dict_baseline_no_updates = eval_baseline_no_updates.evaluate( # recommendation_results_baseline_no_updates\n",
    "    baseline_nodel_not_each_point,\n",
    "    user_average_metrics=False,\n",
    "    retrain_model_each_point=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11691280-1342-4f61-9cdd-5f71cbdcfb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 2.9430047722260126,\n",
       " 'rmse': 3.190256461999607,\n",
       " 'precision': 0.000374970002399808,\n",
       " 'average_precision': 0.13829390681488837,\n",
       " 'mean_reciprocal_rank': 0.0033665109136543046,\n",
       " 'ndcg': 0.9273651300033005,\n",
       " 'coverage': 0.007781456953642384}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_output_dict_baseline_no_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f734968c-80c7-4729-ba20-e28a4ebead12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('baseline_popularity_no_updates_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_output_dict_baseline_no_updates, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e88a11-01ef-47e0-bdcb-9fe66d53d8b3",
   "metadata": {},
   "source": [
    "Absolutely every trend in the difference between the two evaluation approaches is the same here as in the mean-rating baseline model, except `NDCG` decreases more noticeably.\n",
    "\n",
    "One other important thing to point out here is that precision has still decreased from the first evaluation approach, one again showing the evaluation approach with updates is able to suggest the top recommendation slightly more accurately due to having the newest information in the data in the account."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
