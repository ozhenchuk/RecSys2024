{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27542b60-b07a-4cb9-bf00-b00a5500a23d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's develop an evaluation methodology here and then finalize it as the script for the general use in the `src` folder. This approach is also likely to evolve a bit with the complexity of the future models, however, the main principles outlined here will stay the same.\n",
    "\n",
    "One of the most important points in model evaluation here is selecting the data for the train and test datasets. Initial idea for this was to take the whole time range present in the data, and give the latest 20%-30% of it to be the test.\n",
    "\n",
    "However, such an approach has a very serious drawback. As evident from the performed EDA, more than a half of users in our data do not spend long time on the platform and likely login just once. In fact, as fiscovered, almost half of the users have their earliest and latest rating events within an hour. This means that if we split just the general timestamps, than most of these short-period users will have no recommendations to make in test, and around 20-30% that end up in the \"test period\" fully will be essentially the new user predictions.\n",
    "\n",
    "In fact, this is exactly the example how an entensive EDA can later help us to make quick decisions in the later development pipeline steps.\n",
    "\n",
    "So, after reviewing a bit of literature of the topic (the papers [[1]](https://castells.github.io/papers/irj2020.pdf) and [[2]](https://arxiv.org/pdf/2007.13237) were particularly helpful, as they compared several approaches and hinted at the possible solution for our data) and looking at the data properties, the data split strategy was designed.\n",
    "\n",
    "## Data split strategy\n",
    "\n",
    "The motivation behind it is the following one. The temporal structure of our data is one of its most important properties and can contain a lot of information about the user. For the long-term users, sequence of the ratings will most likely means that the user has watched the movies in approximately that order, meaning that the decision to watch the later ones was based on the already watched films. Yes, it is also theoretically possible to recommend earlier movies based on the later ones, but this is less intuitive and natural way of recommending content. A good example would be a user that has watched a film in some new genre, didn't like it at all, and not as a result avoids this genre or this director at all. Therefore, a temporal dimension is important for our data, but if such a need arises it can also be omitted.\n",
    "\n",
    "Interestingly, the short-term users rating sequence also can be indicative of the way they would like to watch them. The user is most likely to first rate the film that made the biggest impact to them, then rate the film that reminds them the previous one the most etc. However, we should be aware that in high-speed rating bursts the previous recommender system that possibly existed on the website from where the ratings are can affect the sequence by making the users rate the suggested movies. This can cause a but of the previous system leakage, but users still usually make a final movie selection themselves, thus \"online validating\" that system.\n",
    "\n",
    "Therefore, the temporal dimension should definitely be present in out split. However, because we don't want the most users to be left out of test altogether and the other being totally new in the face of our system, we need to sample on the user-wise ratings, and not time-wise. For each user, we take their timespan of ratings, and assign the latest 20-30% (as defined by a parameter) to the test. This allows us to make informed test predictions about each user that has enough observations (at least 20 in our case), as is sometimes called a Temporal-User data split.\n",
    "\n",
    "The biggest disadvantage of this method noted in several works is that is samples the same test time period for each user if we \"normalize\" their time on the platform, but their test periods still represent different absolute times. This can cause partial \"future leakage\" for the users other than this, even though it is much less than in the leave-one-out data split. The level of how biased the other user's \"future\" predictions are to this one is devatable, as it can be argued that they just give us more context on how the different users can operate.\n",
    "\n",
    "Also, the danger in \"future leakage\" is in accidentally including the information that is somehow influenced by predictions. However, this is not the case for the other users here. Remember that we have no timestamps of the movies release, and, as such, we have no way of knowing when is the earliest time points when a certain movie can be watched. It does not correspond to the earliest timestamp the movie was rated. Therefore, we predict not \"which of the future films this user can watch\", but \"which of the present in the data films can user watch.\" This is also evident from the fact, that our candidate films will be essentially sampled from the whole dataset. Because of this, the future ratings of *other* users do not really reveal anything about the test predictions of *this* user, as they just select from the whole movie list that we have available for the purpose of our dataset. There newer other user predictions just give more context how the sequence of ratings may go.\n",
    "\n",
    "Worth noting that this would've been completely different if we would've had film release days in our data. In that case, the film list from which any user can select at any given point in time would be different, and would've had no choice but to hard cut the time after the test point's time.\n",
    "\n",
    "Still, this no-release-time notion is artificial, and in the reality films can be realeased alongside our ratings. So even here a very small amount of \"future leakage\" from the other users can occur in the form of the new films, though again it can be argued that there points just provide new context about the users preferences. Most of the approaches from the reviewed literature [2] seem to not take unto account future leakage in the ratings systems at all, or for the rating of the other users.\n",
    "\n",
    "### Evaluation approach algorithm\n",
    "\n",
    "If we want to evaluate how our model would've worked at any point in time, we need to simulate the data as it was at that time. Therefore, for the model that allow such a recalculation, we will do the evaluation this way:\n",
    "\n",
    "- input data: train data, test data, `UserID` to predict, `Timestamp` for which we make a prediction (test point time)\n",
    "- cutoff the updated train data on the test point time (train data + test data that is \"already known\" at that time, example: we trained the model on the data up to 1st June, but the user have since watched 2 other movies that we've also made test on -> the recommendations need to count the previous observation, as with the time series), train the model just on that time\n",
    "- make a prediction based on the capped train data\n",
    "- form such predictions for all of the users\n",
    "- calculate the dataset-wide metrics\n",
    "\n",
    "The second point would be skipped in the future for the models that require lengthy training process (like deep learning approaches), allowing a small part of \"future leakage\" from the other users, as discussed earlier. For that reason, for the other models we will make two evaluations: for time cut-off train data and static train data.\n",
    "\n",
    "The first one is the pure and most realistic model quality evaluation and will be used to compare all models where this approach will be possible. But the second one will be used in the future to compare the results with the models where model updating of the newly observed data for each point would be impossible.\n",
    "\n",
    "## Metrics\n",
    "\n",
    "The following metrics were selected to evaluate the recommendation quality. Each dataset-wide calculation can have several ways of aggregation. For example, metrics can be calculated for every user, or directly for all of the observations in the data. Our evaluation pipeline will have a parameter for this, but by default we do not average over the user's individual scores and calculate it directly. That changed during the development, and the main motivation for this is that the average score over users can \"mask\" a really bad score for some of them.\n",
    "\n",
    "Also, several metrics highly depend on the number of points being evaluated. However, average-over-users metrics are good to confirm the overall performance. The usage really depends on the goal of the recommender system.\n",
    "\n",
    "If we want to equally reward the system for satisfying each user (i.e. likely our profut depends on the number of users), the average is the way to go.\n",
    "\n",
    "If we want to reward the best prediction regardless of the user, meaning that frequent users fill have much more weight (i.e. likely each \"movie watch\" gives us profit instead of a user subscription), than we will use the dataset-wide metric directly.\n",
    "\n",
    "So, the metrics used for the model evaluation are:\n",
    "\n",
    "- MAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443246c2-bb84-40e1-8ab9-a3b70a42a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8909b033-4eaa-45d2-a237-b8f2e322a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b75c51c-a15d-45ce-b8b5-de34f4fdabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv('../../data/ml-1m/ratings.dat',\n",
    "                         delimiter='::',\n",
    "                         header=None,\n",
    "                         names=['UserID','MovieID','Rating','Timestamp'],\n",
    "                         engine ='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04f0fcac-f7be-48dc-bcec-82c2834cf23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_y = df_ratings[df_ratings['UserID'] == 1]['Rating'].tolist()\n",
    "ratings_y_pred_example = np.random.uniform(1, 5, len(ratings_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfd25ce-de51-41e0-a83c-57f0df4ec5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3464283466567633"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(ratings_y, ratings_y_pred_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8789e818-40d2-4f5d-bac9-318a0adf32ee",
   "metadata": {},
   "source": [
    "- RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "494e251a-d5c3-4df0-b63d-638abc459835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad9d2fbc-a4e1-440c-bd73-7d550ac30c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7327511554413404"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(mean_squared_error(ratings_y, ratings_y_pred_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cfa7db-dae2-413c-85a3-be7d59499649",
   "metadata": {},
   "source": [
    "- precision (top-1, always calculated across the entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59274a1-6fd2-444c-aa4a-eef1c1b75d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_special(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ab0193c-0447-4cde-bbf7-83a87251acd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2641509433962264"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_special(ratings_y, np.round(ratings_y_pred_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb5b232-11b1-429c-9ed4-682de1dccd3e",
   "metadata": {},
   "source": [
    "- average precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e216009-c994-4404-a6a7-225741c472fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_top_k(y_true, y_pred, k):\n",
    "    return np.sum(y_true[:k] == y_pred[:k])/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88857a97-aac8-4452-a2d1-e0a2ca48eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(y_true, y_pred, m):\n",
    "    return np.sum([precision_top_k(y_true, y_pred, k) for k in range(1,m+1)])/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0123bd0a-ae22-4611-b80e-7f78e403e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argsort_top_n(y_list, n):\n",
    "    indices_unsorted = np.argpartition(y_list, -n)[-n:]\n",
    "    combined_time_order_unsorted = np.array([np.array(y_list)[indices_unsorted], -indices_unsorted]).T\n",
    "    # print(f'--{combined_time_order_unsorted} {combined_time_order_unsorted[:,::-1].T}')\n",
    "    # print(f'--{np.lexsort(combined_time_order_unsorted[:,::-1].T)}')\n",
    "    # print(f'--{np.lexsort(combined_time_order_unsorted[:,::-1].T)}')\n",
    "    return indices_unsorted[np.lexsort(combined_time_order_unsorted[:,::-1].T)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d26cc361-a03e-42c8-aa24-3fb2f377a1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  6,  7, 10, 14, 18, 22, 23, 25, 36, 37, 39, 40, 41, 45, 46,\n",
       "       48, 17, 19])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argsort_top_n(ratings_y, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1cdebdd-eacd-4f9c-aaa0-053c58ebc82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ratings_y)[argsort_top_n(ratings_y, 20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6592c0-431e-4a01-98dd-6e77b8bc3c22",
   "metadata": {},
   "source": [
    "As we can see, ranking metrics can have a bit of a problem in our data, as ground truth ratings have really small discretization and the ratings of 5 are all really the top-rated ground truth recommendations. Therefore, it only really makes sense to use the average precision and other order- or rank-based approaches for evaluating the results of the top-N predictions, where N is the number of test predictions for the user. Alternatively, we can calculate (unoredered) precision for each user for the top-M predictions, where M is the number of highest-ranked ground truth predictions in the test set for this user.\n",
    "\n",
    "We can theoretically say that the movie the user ranked with 5/5 first would be a better prediction than the next 5/5-ranked movies (as implemented in the `argsort_top_n` function above used to convert the ratings into ranked item predictions), but that is not exactly true. This brings all of the ranked metrics in a bit of a question here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa73ecaa-f238-463b-9633-5e90c043cb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06572031619051742"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision(argsort_top_n(ratings_y, 20),\n",
    "                  argsort_top_n(np.round(ratings_y_pred_example), 20),\n",
    "                  20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad74b6e-489e-4486-b8cc-3db6d44d53ad",
   "metadata": {},
   "source": [
    "- mean reciprocal rank (calculated for each test point based on which position the correct item appears among the ranked predictions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f50d4c30-a4f8-42b6-bf70-69c46a27fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(relevant_items_list, predicted_candidates_lists):\n",
    "    ranks = [list(predicted_candidates_lists[i_i]).index(\n",
    "        item)+1 if item in predicted_candidates_lists[i_i] else 0 for i_i, item in enumerate(relevant_items_list)]\n",
    "    return np.sum([1/rank if rank > 0 else 0 for rank in ranks])/len(relevant_items_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "741cc8fd-cfa1-4119-8bfc-764790ddbed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reciprocal_rank([0, 1, 2], [[1, 2], [1, 2], [1, 2]]) # (0 + 1 + 0.5)/3 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a840ae-f1a1-47c4-ad0f-2b95fd7911a7",
   "metadata": {},
   "source": [
    "- NDCG (calculated for the top-M predictions for the user):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ccd58bf1-206b-4bae-b0b1-44d0ab6ea8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3d389db2-f1db-430c-995b-7c1ba04d8193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_score(np.array(ratings_y)[argsort_top_n(ratings_y, 20)].reshape(1,-1),\n",
    "           np.array(ratings_y_pred_example)[argsort_top_n(ratings_y_pred_example, 20)].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8719f-a627-4aff-8d2b-14defd797490",
   "metadata": {},
   "source": [
    "This demonstrates the issue with the ranked metrics for this specific dataset in the case we have many values with the rating 5: the metrics simply struggle to identify the correct ranking of the ground truth ratings.\n",
    "\n",
    "- Coverage (of the all elements in the train dataset). Is calculated as a simple proportion.\n",
    "\n",
    "Also, as we only have on offline evaluation, our hit rate metric is essentially the same as precision.\n",
    "\n",
    "Now, let's develop a whole evaluation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4ba6a486-217d-4dc7-8671-9b0cd9f6c69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_466877/3060180110.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ratings.groupby('UserID', group_keys=False).apply(lambda x: x.tail(int(np.round(x.shape[0]*0.2))))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>1962</td>\n",
       "      <td>4</td>\n",
       "      <td>978301753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>2692</td>\n",
       "      <td>4</td>\n",
       "      <td>978301570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>1028</td>\n",
       "      <td>5</td>\n",
       "      <td>978301777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>5</td>\n",
       "      <td>978302205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>6040</td>\n",
       "      <td>1091</td>\n",
       "      <td>1</td>\n",
       "      <td>956716541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>6040</td>\n",
       "      <td>1094</td>\n",
       "      <td>5</td>\n",
       "      <td>956704887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>6040</td>\n",
       "      <td>562</td>\n",
       "      <td>5</td>\n",
       "      <td>956704746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>6040</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "      <td>956715648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>6040</td>\n",
       "      <td>1097</td>\n",
       "      <td>4</td>\n",
       "      <td>956715569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200016 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID  MovieID  Rating  Timestamp\n",
       "42            1     1962       4  978301753\n",
       "43            1     2692       4  978301570\n",
       "44            1      260       4  978300760\n",
       "45            1     1028       5  978301777\n",
       "46            1     1029       5  978302205\n",
       "...         ...      ...     ...        ...\n",
       "1000204    6040     1091       1  956716541\n",
       "1000205    6040     1094       5  956704887\n",
       "1000206    6040      562       5  956704746\n",
       "1000207    6040     1096       4  956715648\n",
       "1000208    6040     1097       4  956715569\n",
       "\n",
       "[200016 rows x 4 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.groupby('UserID', group_keys=False).apply(lambda x: x.tail(int(np.round(x.shape[0]*0.2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b7522283-6a48-4e51-9e06-c1eab58575cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MovieID  Rating  Timestamp\n",
       "UserID                            \n",
       "1            53      53         53\n",
       "2           129     129        129\n",
       "3            51      51         51\n",
       "4            21      21         21\n",
       "5           198     198        198"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.groupby('UserID').count().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a1438d1f-8576-4e21-9a64-38ad8ac6a184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_466877/3546730996.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ratings.groupby('UserID', group_keys=False).apply(lambda x: x.tail(int(np.round(x.shape[0]*0.2)))).groupby('UserID').count().head()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MovieID  Rating  Timestamp\n",
       "UserID                            \n",
       "1            11      11         11\n",
       "2            26      26         26\n",
       "3            10      10         10\n",
       "4             4       4          4\n",
       "5            40      40         40"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.groupby('UserID', group_keys=False).apply(lambda x: x.tail(int(np.round(x.shape[0]*0.2)))).groupby('UserID').count().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "66b627e3-ee99-408c-b1ec-67893556ffa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserID               1\n",
       "MovieID           1962\n",
       "Rating               4\n",
       "Timestamp    978301753\n",
       "Name: 42, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.loc[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31616cb-fe89-411e-a831-b65d119f3cf4",
   "metadata": {},
   "source": [
    "The pipeline below was a result of extensive development and experimentation and took almost 70% of the time spent on taskts 1 and 2 of the first submission. Here is just the final version which is also doubled in the `src/evaluation.py`, most of the development process took place in parallel to the mean-rating baseline model development as they allow to test each other, so it can be reviewed in the `Exp_1_4` folder under the name `experiment_dev_evaluation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8135e8af-8d0c-44da-933b-9a32f7032714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationPipeline:\n",
    "    def __init__(self,\n",
    "                 total_rating_data,\n",
    "                 train_test_split: float = 0.2):\n",
    "        self.total_rating_data = total_rating_data\n",
    "        self.train_test_split = train_test_split\n",
    "        self.test_data = self.get_test_data(self.total_rating_data)\n",
    "        self.train_data = self.total_rating_data[~self.total_rating_data.index.isin(self.test_data.index)]\n",
    "\n",
    "    def get_test_data(self, total_data):\n",
    "        return total_data.groupby('UserID', group_keys=False).apply(\n",
    "            lambda x: x.tail(int(np.round(x.shape[0]*self.train_test_split))))\n",
    "\n",
    "    def evaluate(self,\n",
    "                 model_object,\n",
    "                 metrics_list = False,\n",
    "                 user_average_metrics: bool = False,\n",
    "                 retrain_model_each_point: bool = False):\n",
    "        if not metrics_list:\n",
    "            metrics_list = ['mae','rmse','precision','average_precision',\n",
    "                            'mean_reciprocal_rank','ndcg','coverage']\n",
    "        if user_average_metrics:\n",
    "            recommendation_results = {}\n",
    "            ratings_y_pred = {}\n",
    "        else:\n",
    "            recommendation_results = []\n",
    "            ratings_y_pred = []\n",
    "        sorted_total_data_test = self.total_rating_data.sort_values('Timestamp')\n",
    "        rows_before_timestamp_test = sorted_total_data_test.groupby('Timestamp').count()[\n",
    "            'UserID'].cumsum().shift(1).fillna(0).astype(int).to_dict()\n",
    "        if retrain_model_each_point:\n",
    "            sorted_data_train = self.train_data.sort_values('Timestamp')\n",
    "            rows_before_timestamp_train = sorted_data_train.groupby('Timestamp').count()[\n",
    "                'UserID'].cumsum().shift(1).fillna(0).astype(int)\n",
    "            timestamps_not_in_train = [timestamp for timestamp in rows_before_timestamp_test.keys(\n",
    "                ) if timestamp not in rows_before_timestamp_train.index]\n",
    "            rows_before_timestamp_train = pd.concat([rows_before_timestamp_train,\n",
    "                                                     pd.Series(None,\n",
    "                                                               index=timestamps_not_in_train)])\n",
    "            rows_before_timestamp_train = rows_before_timestamp_train.sort_index().ffill().astype(int)\n",
    "        for i_p, test_point in tqdm(self.test_data.iterrows(),\n",
    "                                    total=self.test_data.shape[0]):\n",
    "            test_point_timestamp = test_point['Timestamp']\n",
    "            test_point_user = test_point['UserID']\n",
    "            if user_average_metrics and test_point_user not in recommendation_results.keys():\n",
    "                recommendation_results[test_point_user] = []\n",
    "                ratings_y_pred[test_point_user] = []\n",
    "            if retrain_model_each_point:\n",
    "                train_data_each_point = sorted_data_train[:rows_before_timestamp_train[test_point_timestamp]]\n",
    "                model_object.fit(train_data_each_point)\n",
    "            # Using the whole dataset, as for each new test point for the user previous test points matter too\n",
    "            items_pred, ratings_pred = model_object.predict(sorted_total_data_test[\n",
    "                                                                :rows_before_timestamp_test[test_point_timestamp]],\n",
    "                                                            test_point_user,\n",
    "                                                            test_point_timestamp)\n",
    "            if user_average_metrics:\n",
    "                recommendation_results[test_point_user].append([i_p, items_pred])\n",
    "                ratings_y_pred[test_point_user].append(ratings_pred[items_pred.tolist().index(\n",
    "                    test_point_user)] if (test_point_user in items_pred) else 0)\n",
    "            else:\n",
    "                recommendation_results.append([i_p, items_pred])\n",
    "                ratings_y_pred.append(ratings_pred[items_pred.tolist().index(\n",
    "                    test_point_user)] if (test_point_user in items_pred) else 0)\n",
    "        metrics_output_dict = {}\n",
    "        if not user_average_metrics:\n",
    "            ratings_y_true = [self.test_data.loc[\n",
    "                              test_point_index, 'Rating'] for test_point_index in self.test_data.index]\n",
    "            # ratings_y_pred = [ratings_pred[items_pred.index(\n",
    "            #     self.test_data.loc[test_point_index, 'movieID'])] if self.test_data.loc[\n",
    "            #         test_point_index, 'movieID'] in items_pred else 0 for test_point_index in self.test_data.index]\n",
    "            # ratings_y_pred = [recommendation_results[i_p][2][recommendation_results[i_p][1].tolist().index(\n",
    "            #     self.test_data.loc[test_point_index, 'MovieID'])] if self.test_data.loc[\n",
    "            #         test_point_index, 'MovieID'] in recommendation_results[i_p][\n",
    "            #             1] else 0 for i_p, test_point_index in enumerate(self.test_data.index)]\n",
    "            self.test_data['Rating pred'] = ratings_y_pred\n",
    "            ratings_y_true_users = self.test_data.groupby('UserID')['Rating'].apply(list).to_dict()\n",
    "            ratings_y_pred_users = self.test_data.groupby('UserID')['Rating pred'].apply(list).to_dict()\n",
    "            largest_user_id_total = self.total_rating_data['UserID'].max()\n",
    "            items_id_pred = [pred[1][0] if len(pred[1]) > 0 else (\n",
    "                largest_user_id_total + 1) for pred in recommendation_results]\n",
    "            for metric in metrics_list:\n",
    "                if metric == 'mae':\n",
    "                    metrics_output_dict['mae'] = mean_absolute_error(ratings_y_true, ratings_y_pred)\n",
    "                elif metric == 'rmse':\n",
    "                    metrics_output_dict['rmse'] = sqrt(mean_squared_error(ratings_y_true, ratings_y_pred))\n",
    "                elif metric == 'precision':\n",
    "                    metrics_output_dict['precision'] = precision_special(self.test_data['MovieID'].to_numpy(),\n",
    "                                                                         items_id_pred)\n",
    "                elif metric == 'average_precision':\n",
    "                    average_precision_list = []\n",
    "                    for user in self.test_data['UserID'].unique():\n",
    "                        m_user = len(ratings_y_true_users[user])\n",
    "                        average_precision_list.append(average_precision(\n",
    "                            argsort_top_n(ratings_y_true_users[user], m_user),\n",
    "                            argsort_top_n(ratings_y_pred_users[user], m_user),\n",
    "                            m_user))\n",
    "                    metrics_output_dict['average_precision'] = np.mean(average_precision_list)\n",
    "                elif metric == 'mean_reciprocal_rank':\n",
    "                    metrics_output_dict['mean_reciprocal_rank'] = mean_reciprocal_rank(\n",
    "                        self.test_data['MovieID'].to_numpy(),\n",
    "                        [pred[1] for pred in recommendation_results])\n",
    "                elif metric == 'ndcg':\n",
    "                    ndcg = []\n",
    "                    for user in self.test_data['UserID'].unique():\n",
    "                        m_user = len(ratings_y_true_users[user])\n",
    "                        if m_user > 1:\n",
    "                            # NDCG only is defined is there is more than 1 point\n",
    "                            ndcg.append(ndcg_score(\n",
    "                                np.array(ratings_y_true_users[user])[\n",
    "                                    argsort_top_n(ratings_y_true_users[user], m_user)].reshape(1,-1),\n",
    "                                np.array(ratings_y_pred_users[user])[\n",
    "                                    argsort_top_n(ratings_y_pred_users[user], m_user)].reshape(1,-1)))\n",
    "                    if len(ndcg) > 0:\n",
    "                        metrics_output_dict['ndcg'] = np.mean(ndcg)\n",
    "                    else:\n",
    "                        metrics_output_dict['ndcg'] = 0.0\n",
    "                elif metric == 'coverage':\n",
    "                    items_train_unique = self.train_data['UserID'].unique()\n",
    "                    metrics_output_dict['coverage'] = len(np.unique([\n",
    "                        item for item in items_id_pred if item in items_train_unique]))/len(items_train_unique)\n",
    "        return metrics_output_dict # recommendation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8e00a80a-6d5e-498e-84c6-bfe6ddd93e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "PROJECT_DIR = '../../'\n",
    "PROJECT_DIR = osp.abspath(PROJECT_DIR)\n",
    "print(PROJECT_DIR in sys.path)\n",
    "if PROJECT_DIR not in sys.path:\n",
    "    print(f'Adding project directory to the sys.path: {PROJECT_DIR!r}')\n",
    "    sys.path.insert(1, PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "363dd650-5bb6-4275-8450-b3489e4a4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import EvaluationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bb450771-ab74-40fa-bd37-220352d2cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_baseline = EvaluationPipeline(df_ratings, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dc6455ed-e1c4-44d2-9c54-fd08e966b16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.evaluation.EvaluationPipeline at 0x7fd406a73ca0>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f2f78-67cc-4f19-aa3b-319c100aa8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca9fa2-89a8-4aca-b6bb-90e7c56781bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8294f71-372d-4691-beb6-6b9e76ba8ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160bd71e-53e4-471e-8ab1-fa86c5a32140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
